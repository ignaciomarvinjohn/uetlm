# U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models

This is the main repository for the paper "U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models". We will provide the code once the paper is published.

Key Contributions:
- New architecture that reduces the computational complexity of Transformer-based language models by reducing the embedding size while maintaining competitive performance
- Practical training approach to training language models in a resource-constrained environment

# Sample of the Generated Texts

Kernel 7 | Original Embedding Reduction
<table>
<tr>
<td>

<pre>
TinyShakespeare
- Hello.
- How are you?
</pre>

</td>
<td>

<pre>
WikiText
- I'm fine.
- Thanks for asking!
</pre>

</td>
<td>

<pre>
Bookcorpus
- I'm fine.
- Thanks for asking!
</pre>

</td>
</tr>
</table>



Related papers:
- **Meme Analysis using LLM-based Contextual Information and U-net Encapsulated Transformer** | [paper](https://ieeexplore.ieee.org/document/10589379) | [Github](https://github.com/ignaciomarvinjohn/meme-uet-hmt)
- **UET4Rec**: U-net encapsulated transformer for sequential recommender | [paper](https://www.sciencedirect.com/science/article/pii/S0957417424016488) | [Github](https://github.com/ignaciomarvinjohn/uet4rec)
