# U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models

This is the main repository for the paper "U-Net Encapsulated Transformer for Reducing Dimensionality in Training Large Language Models". We will provide the code once the paper is published.

Key Contributions:
- New architecture that reduces the computational complexity of Transformer-based language models by reducing the embedding size while maintaining competitive performance
- Practical training approach to training language models in a resource-constrained environment

Meant for researchers and AI developers aiming to conduct a study or apply language models to their work. Training a language model from scratch using limited hardware is now possible!

Related papers:
- **Meme Analysis using LLM-based Contextual Information and U-net Encapsulated Transformer** | [paper](https://ieeexplore.ieee.org/document/10589379) | [Github](https://github.com/ignaciomarvinjohn/meme-uet-hmt)
- **UET4Rec**: U-net encapsulated transformer for sequential recommender | [paper](https://www.sciencedirect.com/science/article/pii/S0957417424016488) | [Github](https://github.com/ignaciomarvinjohn/uet4rec)

Stay tuned!
